{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ThreadTheRipper\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                  text  \\\n0    The accelerator programs have sub-portfolios o...   \n1    Also by means of BNDES Finem, we offer credit ...   \n2    Climate change Climate change exposes UPM to v...   \n3    Several tools and methodologies aimed at asses...   \n4    We worked with the UK government to accelerate...   \n..                                                 ...   \n395  At the beginning of 2019, VINCI Airports signe...   \n396  We have also signed up to the Partnership for ...   \n397  Suzano also is involved and spearheads externa...   \n398  Risks to the Group’s reputation Risks include ...   \n399  UBS is also involved in other activities to re...   \n\n                                                tokens  \n0    [the, accelerator, programs, have, of, focused...  \n1    [also, by, means, of, bndes, finem, we, offer,...  \n2    [climate, change, climate, change, exposes, up...  \n3    [several, tools, and, methodologies, aimed, at...  \n4    [we, worked, with, the, uk, government, to, ac...  \n..                                                 ...  \n395  [at, the, beginning, of, 2019, vinci, airports...  \n396  [we, have, also, signed, up, to, the, partners...  \n397  [suzano, also, is, involved, and, spearheads, ...  \n398  [risks, to, the, group, s, reputation, risks, ...  \n399  [ubs, is, also, involved, in, other, activitie...  \n\n[400 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The accelerator programs have sub-portfolios o...</td>\n      <td>[the, accelerator, programs, have, of, focused...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Also by means of BNDES Finem, we offer credit ...</td>\n      <td>[also, by, means, of, bndes, finem, we, offer,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Climate change Climate change exposes UPM to v...</td>\n      <td>[climate, change, climate, change, exposes, up...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Several tools and methodologies aimed at asses...</td>\n      <td>[several, tools, and, methodologies, aimed, at...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>We worked with the UK government to accelerate...</td>\n      <td>[we, worked, with, the, uk, government, to, ac...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>395</th>\n      <td>At the beginning of 2019, VINCI Airports signe...</td>\n      <td>[at, the, beginning, of, 2019, vinci, airports...</td>\n    </tr>\n    <tr>\n      <th>396</th>\n      <td>We have also signed up to the Partnership for ...</td>\n      <td>[we, have, also, signed, up, to, the, partners...</td>\n    </tr>\n    <tr>\n      <th>397</th>\n      <td>Suzano also is involved and spearheads externa...</td>\n      <td>[suzano, also, is, involved, and, spearheads, ...</td>\n    </tr>\n    <tr>\n      <th>398</th>\n      <td>Risks to the Group’s reputation Risks include ...</td>\n      <td>[risks, to, the, group, s, reputation, risks, ...</td>\n    </tr>\n    <tr>\n      <th>399</th>\n      <td>UBS is also involved in other activities to re...</td>\n      <td>[ubs, is, also, involved, in, other, activitie...</td>\n    </tr>\n  </tbody>\n</table>\n<p>400 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "nltk.download('punkt')\n",
    "\n",
    "path = str(Path.cwd()) + '\\project_training.json'\n",
    "# print(path)\n",
    "# Read the JSON file\n",
    "with open(path, 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "# Load the JSON data into a dataframe\n",
    "df_train = pd.read_json(data)\n",
    "\n",
    "# Create a dataframes for the text and the climate label\n",
    "df_text = pd.DataFrame(df_train, columns=['text'])\n",
    "df_climate = pd.DataFrame(df_train, columns=['climate'])\n",
    "# Keep rows where df_climate is 'yes'\n",
    "df_filtered = df_train.loc[df_climate['climate'] == 'yes']\n",
    "\n",
    "# Split the filtered dataframe into separate dataframes\n",
    "df_text_climate_yes = pd.DataFrame(df_filtered, columns=['text'])\n",
    "df_sentiment = pd.DataFrame(df_filtered, columns=['sentiment'])\n",
    "df_commitment = pd.DataFrame(df_filtered, columns=['commitment'])\n",
    "df_specificity = pd.DataFrame(df_filtered, columns=['specificity'])\n",
    "# turn all labels into numerical labels\n",
    "\n",
    "df_climate = df_climate.replace({'yes': 1, 'no': 0})\n",
    "# opportunity/neutral/risk\n",
    "df_sentiment = df_sentiment.replace({'opportunity': 0, 'neutral': 1, 'risk': 2})\n",
    "# yes/no\n",
    "df_commitment = df_commitment.replace({'yes': 1, 'no': 0})\n",
    "# specific language/non-specific language\n",
    "df_specificity = df_specificity.replace({'spec': 1, 'non': 0})\n",
    "\n",
    "path = str(Path.cwd()) + '\\project_validation.json'\n",
    "with open(path, 'r') as f_test:\n",
    "    data_test = f_test.read()\n",
    "\n",
    "# Load the JSON data into a dataframe\n",
    "df_test = pd.read_json(data_test)\n",
    "df_text_test = pd.DataFrame(df_test, columns=['text'])\n",
    "df_climate_test = pd.DataFrame(df_test, columns=['climate'])\n",
    "# Keep rows where df_climate is 'yes'\n",
    "df_filtered_test = df_test.loc[df_climate_test['climate'] == 'yes']\n",
    "df_text_test_climate_yes = pd.DataFrame(df_filtered_test, columns=['text'])\n",
    "df_sentiment_test = pd.DataFrame(df_filtered_test, columns=['sentiment'])\n",
    "df_commitment_test = pd.DataFrame(df_filtered_test, columns=['commitment'])\n",
    "df_specificity_test = pd.DataFrame(df_filtered_test, columns=['specificity'])\n",
    "# same for climate classification text data\n",
    "df_climate_test = df_climate_test.replace({'yes': 1, 'no': 0})\n",
    "# opportunity/neutral/risk\n",
    "df_sentiment_test = df_sentiment_test.replace({'opportunity': 0, 'neutral': 1, 'risk': 2})\n",
    "# yes/no\n",
    "df_commitment_test = df_commitment_test.replace({'yes': 1, 'no': 0})\n",
    "# specific language/non-specific language\n",
    "df_specificity_test = df_specificity_test.replace({'spec': 1, 'non': 0})\n",
    "\n",
    "def lowercase_delete_special_characters(tokens):\n",
    "    modified_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.isalpha():\n",
    "            modified_tokens.append(token.lower())\n",
    "        elif token.isnumeric():\n",
    "            modified_tokens.append(token)\n",
    "    return modified_tokens\n",
    "\n",
    "\n",
    "df_text['tokens'] = df_text['text'].apply(nltk.word_tokenize)\n",
    "df_text['tokens'] = df_text['tokens'].apply(lowercase_delete_special_characters)\n",
    "\n",
    "df_text_climate_yes['tokens'] = df_text_climate_yes['text'].apply(nltk.word_tokenize)\n",
    "df_text_climate_yes['tokens'] = df_text_climate_yes['tokens'].apply(lowercase_delete_special_characters)\n",
    "\n",
    "df_text_test['tokens'] = df_text_test['text'].apply(nltk.word_tokenize)\n",
    "df_text_test['tokens'] = df_text_test['tokens'].apply(lowercase_delete_special_characters)\n",
    "\n",
    "df_text_test_climate_yes['tokens'] = df_text_test_climate_yes['text'].apply(nltk.word_tokenize)\n",
    "df_text_test_climate_yes['tokens'] = df_text_test_climate_yes['tokens'].apply(lowercase_delete_special_characters)\n",
    "\n",
    "df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_word2vec = KeyedVectors.load_word2vec_format('glove.6B.300d.txt', binary=False, no_header=True)\n",
    "# print(model.most_similar(positive=['sustainability']))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "def document_vector_func(doc):\n",
    "    # Create document vectors by averaging word vectors, Remove out-of-vocabulary words\n",
    "    doc = [model_word2vec.get_vector(word) for word in doc if word in model_word2vec.index_to_key]\n",
    "    doc = np.vstack(doc)\n",
    "    return np.mean(doc, axis=0)\n",
    "\n",
    "df_text['document_vectors'] = df_text['tokens'].apply(document_vector_func)\n",
    "df_text_test['document_vectors'] = df_text_test['tokens'].apply(document_vector_func)\n",
    "df_text_climate_yes['document_vectors'] = df_text_climate_yes['tokens'].apply(document_vector_func)\n",
    "df_text_test_climate_yes['document_vectors'] = df_text_test_climate_yes['tokens'].apply(document_vector_func)\n",
    "\n",
    "document_vectors = df_text.document_vectors.tolist()\n",
    "document_vectors_test = df_text_test.document_vectors.tolist()\n",
    "document_vectors_climate_yes = df_text_climate_yes.document_vectors.tolist()\n",
    "document_vectors_test_climate_yes = df_text_test_climate_yes.document_vectors.tolist()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "# save all document vectors into one dataframe for each tasks\n",
    "# cross-validation means that train and validation data doesn't need to be kept separate\n",
    "\n",
    "document_vectors_all = document_vectors + document_vectors_test\n",
    "document_vectors_climate_yes_all = document_vectors_climate_yes + document_vectors_test_climate_yes\n",
    "\n",
    "df_climate_all = pd.concat([df_climate, df_climate_test])\n",
    "df_sentiment_all = pd.concat([df_sentiment, df_sentiment_test])\n",
    "df_commitment_all = pd.concat([df_commitment, df_commitment_test])\n",
    "df_specificity_all = pd.concat([df_specificity, df_specificity_test])\n",
    "\n",
    "df_climate_all['document_vector'] = document_vectors_all\n",
    "df_climate_all = df_climate_all.iloc[:,[1,0]]\n",
    "\n",
    "df_sentiment_all['document_vector'] = document_vectors_climate_yes_all\n",
    "df_sentiment_all = df_sentiment_all.iloc[:,[1,0]]\n",
    "\n",
    "df_commitment_all['document_vector'] = document_vectors_climate_yes_all\n",
    "df_commitment_all = df_commitment_all.iloc[:,[1,0]]\n",
    "\n",
    "df_specificity_all['document_vector'] = document_vectors_climate_yes_all\n",
    "df_specificity_all = df_specificity_all.iloc[:,[1,0]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.87      0.81        30\n",
      "           1       0.97      0.94      0.95       130\n",
      "\n",
      "    accuracy                           0.93       160\n",
      "   macro avg       0.87      0.90      0.88       160\n",
      "weighted avg       0.93      0.93      0.93       160\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.85      0.78        27\n",
      "           1       0.97      0.93      0.95       133\n",
      "\n",
      "    accuracy                           0.92       160\n",
      "   macro avg       0.84      0.89      0.86       160\n",
      "weighted avg       0.93      0.92      0.92       160\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.85      0.71        26\n",
      "           1       0.97      0.90      0.93       134\n",
      "\n",
      "    accuracy                           0.89       160\n",
      "   macro avg       0.79      0.87      0.82       160\n",
      "weighted avg       0.91      0.89      0.89       160\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.79      0.63        29\n",
      "           1       0.95      0.84      0.89       131\n",
      "\n",
      "    accuracy                           0.83       160\n",
      "   macro avg       0.74      0.82      0.76       160\n",
      "weighted avg       0.87      0.83      0.84       160\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.89      0.73        27\n",
      "           1       0.98      0.89      0.93       133\n",
      "\n",
      "    accuracy                           0.89       160\n",
      "   macro avg       0.80      0.89      0.83       160\n",
      "weighted avg       0.91      0.89      0.90       160\n",
      "\n",
      "average precision: 0.8060907258952993\n",
      "average recall: 0.8739892157027356\n",
      "average f1: 0.8312619396275878\n",
      "average accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# climate classification\n",
    "\n",
    "results = []\n",
    "accuracy = []\n",
    "\n",
    "# cross-validation with KFold 5-times, 5Fold\n",
    "n=5\n",
    "# initialize KFold\n",
    "kf = KFold(n_splits=n, random_state=72, shuffle=True)\n",
    "\n",
    "# Kfold can then be used to create the five different training and test sets\n",
    "for train_index, test_index in kf.split(df_climate_all):\n",
    "    train_documents = df_climate_all.iloc[train_index]\n",
    "    test_documents = df_climate_all.iloc[test_index]\n",
    "    # Unoptimized LR\n",
    "    # m = LogisticRegression(penalty=None, max_iter=300)\n",
    "    # Optimized LR\n",
    "    m = LogisticRegression(penalty='l2', max_iter=300, class_weight='balanced', C=8.0)\n",
    "    m.fit(train_documents['document_vector'].values.tolist(), train_documents.climate)\n",
    "\n",
    "    # sns.histplot(m.coef_[0], kde=True, binwidth=0.1)\n",
    "\n",
    "    pred_class = m.predict(test_documents['document_vector'].values.tolist())\n",
    "\n",
    "    print(classification_report(test_documents.climate, pred_class))\n",
    "    # append the metrics to an array\n",
    "    results.append(precision_recall_fscore_support(test_documents.climate, pred_class, average='macro'))\n",
    "    accuracy.append(accuracy_score(test_documents.climate, pred_class))\n",
    "\n",
    "# calculate the average metrics\n",
    "avg_precision = np.mean([results[0][0], results[1][0], results[2][0], results[3][0], results[4][0]])\n",
    "avg_recall = np.mean([results[0][1], results[1][1], results[2][1], results[3][1], results[4][1]])\n",
    "avg_f = np.mean([results[0][2], results[1][2], results[2][2], results[3][2], results[4][2]])\n",
    "avg_acc = np.mean(accuracy)\n",
    "\n",
    "print(f\"average precision: {avg_precision}\")\n",
    "print(f\"average recall: {avg_recall}\")\n",
    "print(f\"average f1: {avg_f}\")\n",
    "print(f\"average accuracy: {avg_acc}\")\n",
    "\n",
    "# save them in a format so that they can be stored in an excel file using pandas\n",
    "result_climate = {\"macro avg\":{\"average precision\" : avg_precision,\n",
    "                               \"average recall\" : avg_recall,\n",
    "                               \"average f1\" : avg_f,\n",
    "                               \"average accuracy\" : avg_acc\n",
    "                               }}\n",
    "result_climate = pd.DataFrame(result_climate).transpose()\n",
    "with pd.ExcelWriter(\"metrics_new.xlsx\", mode=\"a\", engine=\"openpyxl\", if_sheet_exists='replace') as writer:\n",
    "    result_climate.to_excel(writer, sheet_name=\"glove_climate_LR\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.65      0.65        40\n",
      "           1       0.70      0.73      0.71        55\n",
      "           2       0.92      0.87      0.89        38\n",
      "\n",
      "    accuracy                           0.74       133\n",
      "   macro avg       0.76      0.75      0.75       133\n",
      "weighted avg       0.75      0.74      0.75       133\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.59      0.68        32\n",
      "           1       0.73      0.88      0.80        51\n",
      "           2       0.96      0.90      0.93        49\n",
      "\n",
      "    accuracy                           0.82       132\n",
      "   macro avg       0.82      0.79      0.80       132\n",
      "weighted avg       0.83      0.82      0.82       132\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.66      0.77        35\n",
      "           1       0.66      0.87      0.75        47\n",
      "           2       0.91      0.82      0.86        50\n",
      "\n",
      "    accuracy                           0.80       132\n",
      "   macro avg       0.83      0.78      0.79       132\n",
      "weighted avg       0.82      0.80      0.80       132\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.86      0.88        37\n",
      "           1       0.85      0.85      0.85        53\n",
      "           2       0.91      0.93      0.92        42\n",
      "\n",
      "    accuracy                           0.88       132\n",
      "   macro avg       0.88      0.88      0.88       132\n",
      "weighted avg       0.88      0.88      0.88       132\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.80      0.86        30\n",
      "           1       0.82      0.90      0.86        60\n",
      "           2       0.85      0.81      0.83        42\n",
      "\n",
      "    accuracy                           0.85       132\n",
      "   macro avg       0.86      0.84      0.85       132\n",
      "weighted avg       0.85      0.85      0.85       132\n",
      "\n",
      "average precision: 0.8313998881227066\n",
      "average recall: 0.8080837262775138\n",
      "average f1: 0.8151075425954606\n",
      "average accuracy: 0.817053998632946\n"
     ]
    }
   ],
   "source": [
    "# sentiment classification\n",
    "\n",
    "results = []\n",
    "accuracy = []\n",
    "\n",
    "n=5\n",
    "kf = KFold(n_splits=n, random_state=72, shuffle=True)\n",
    "\n",
    "for train_index, test_index in kf.split(df_sentiment_all):\n",
    "    train_documents = df_sentiment_all.iloc[train_index]\n",
    "    test_documents = df_sentiment_all.iloc[test_index]\n",
    "    # Unoptimized LR\n",
    "    # m_sent = LogisticRegression(penalty=None, max_iter=300)\n",
    "    # Optimized LR\n",
    "    m_sent = LogisticRegression(penalty='l2', max_iter=300)\n",
    "    m_sent.fit(train_documents['document_vector'].values.tolist(), train_documents.sentiment)\n",
    "\n",
    "    # sns.histplot(m_sent.coef_[0], kde=True, binwidth=0.1)\n",
    "\n",
    "    pred_class = m_sent.predict(test_documents['document_vector'].values.tolist())\n",
    "\n",
    "    print(classification_report(test_documents.sentiment, pred_class))\n",
    "    results.append(precision_recall_fscore_support(test_documents.sentiment, pred_class, average='macro'))\n",
    "    accuracy.append(accuracy_score(test_documents.sentiment, pred_class))\n",
    "\n",
    "avg_precision = np.mean([results[0][0], results[1][0], results[2][0], results[3][0], results[4][0]])\n",
    "avg_recall = np.mean([results[0][1], results[1][1], results[2][1], results[3][1], results[4][1]])\n",
    "avg_f = np.mean([results[0][2], results[1][2], results[2][2], results[3][2], results[4][2]])\n",
    "avg_acc = np.mean(accuracy)\n",
    "\n",
    "print(f\"average precision: {avg_precision}\")\n",
    "print(f\"average recall: {avg_recall}\")\n",
    "print(f\"average f1: {avg_f}\")\n",
    "print(f\"average accuracy: {avg_acc}\")\n",
    "\n",
    "result_sent = {\"macro avg\":{\"average precision\" : avg_precision,\n",
    "                               \"average recall\" : avg_recall,\n",
    "                               \"average f1\" : avg_f,\n",
    "                               \"average accuracy\" : avg_acc\n",
    "                               }}\n",
    "result_sent = pd.DataFrame(result_sent).transpose()\n",
    "with pd.ExcelWriter(\"metrics_new.xlsx\", mode=\"a\", engine=\"openpyxl\", if_sheet_exists='replace') as writer:\n",
    "    result_sent.to_excel(writer, sheet_name=\"glove_sentiment_LR\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.78      0.83        74\n",
      "           1       0.76      0.86      0.81        59\n",
      "\n",
      "    accuracy                           0.82       133\n",
      "   macro avg       0.82      0.82      0.82       133\n",
      "weighted avg       0.83      0.82      0.82       133\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86        72\n",
      "           1       0.84      0.80      0.82        60\n",
      "\n",
      "    accuracy                           0.84       132\n",
      "   macro avg       0.84      0.84      0.84       132\n",
      "weighted avg       0.84      0.84      0.84       132\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.91      0.85        70\n",
      "           1       0.88      0.73      0.80        62\n",
      "\n",
      "    accuracy                           0.83       132\n",
      "   macro avg       0.84      0.82      0.82       132\n",
      "weighted avg       0.83      0.83      0.82       132\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.81      0.87        72\n",
      "           1       0.80      0.95      0.87        60\n",
      "\n",
      "    accuracy                           0.87       132\n",
      "   macro avg       0.88      0.88      0.87       132\n",
      "weighted avg       0.88      0.87      0.87       132\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.84      0.86        82\n",
      "           1       0.76      0.82      0.79        50\n",
      "\n",
      "    accuracy                           0.83       132\n",
      "   macro avg       0.82      0.83      0.83       132\n",
      "weighted avg       0.84      0.83      0.83       132\n",
      "\n",
      "average precision: 0.8392074787177355\n",
      "average recall: 0.838030169953312\n",
      "average f1: 0.835326420917031\n",
      "average accuracy: 0.8381521986785143\n"
     ]
    }
   ],
   "source": [
    "# commitment classification\n",
    "\n",
    "results = []\n",
    "accuracy = []\n",
    "\n",
    "n=5\n",
    "kf = KFold(n_splits=n, random_state=72, shuffle=True)\n",
    "# penalty='l1', C=1.0, solver='liblinear'\n",
    "# penalty='none'\n",
    "\n",
    "# penalty='l2', C=1.0, max_iter=100\n",
    "for train_index, test_index in kf.split(df_commitment_all):\n",
    "    train_documents = df_commitment_all.iloc[train_index]\n",
    "    test_documents = df_commitment_all.iloc[test_index]\n",
    "    # Unoptimized LR\n",
    "    # m_com = LogisticRegression(penalty=None, max_iter=300)\n",
    "    # Optimized LR\n",
    "    m_com = LogisticRegression(penalty='l2', max_iter=300, C=5.0)\n",
    "    m_com.fit(train_documents['document_vector'].values.tolist(), train_documents.commitment)\n",
    "\n",
    "    # sns.histplot(m_com.coef_[0], kde=True, binwidth=0.1)\n",
    "\n",
    "    pred_class = m_com.predict(test_documents['document_vector'].values.tolist())\n",
    "\n",
    "    print(classification_report(test_documents.commitment, pred_class))\n",
    "    results.append(precision_recall_fscore_support(test_documents.commitment, pred_class, average='macro'))\n",
    "    accuracy.append(accuracy_score(test_documents.commitment, pred_class))\n",
    "\n",
    "avg_precision = np.mean([results[0][0], results[1][0], results[2][0], results[3][0], results[4][0]])\n",
    "avg_recall = np.mean([results[0][1], results[1][1], results[2][1], results[3][1], results[4][1]])\n",
    "avg_f = np.mean([results[0][2], results[1][2], results[2][2], results[3][2], results[4][2]])\n",
    "avg_acc = np.mean(accuracy)\n",
    "\n",
    "print(f\"average precision: {avg_precision}\")\n",
    "print(f\"average recall: {avg_recall}\")\n",
    "print(f\"average f1: {avg_f}\")\n",
    "print(f\"average accuracy: {avg_acc}\")\n",
    "\n",
    "result_commitment = {\"macro avg\":{\"average precision\" : avg_precision,\n",
    "                            \"average recall\" : avg_recall,\n",
    "                            \"average f1\" : avg_f,\n",
    "                            \"average accuracy\" : avg_acc\n",
    "                            }}\n",
    "result_commitment = pd.DataFrame(result_commitment).transpose()\n",
    "with pd.ExcelWriter(\"metrics_new.xlsx\", mode=\"a\", engine=\"openpyxl\", if_sheet_exists='replace') as writer:\n",
    "    result_commitment.to_excel(writer, sheet_name=\"glove_commitment_LR\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.83      0.85        72\n",
      "           1       0.81      0.85      0.83        61\n",
      "\n",
      "    accuracy                           0.84       133\n",
      "   macro avg       0.84      0.84      0.84       133\n",
      "weighted avg       0.84      0.84      0.84       133\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.88      0.83        74\n",
      "           1       0.82      0.69      0.75        58\n",
      "\n",
      "    accuracy                           0.80       132\n",
      "   macro avg       0.80      0.78      0.79       132\n",
      "weighted avg       0.80      0.80      0.79       132\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.91      0.89        82\n",
      "           1       0.84      0.76      0.80        50\n",
      "\n",
      "    accuracy                           0.86       132\n",
      "   macro avg       0.85      0.84      0.84       132\n",
      "weighted avg       0.86      0.86      0.85       132\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.84        78\n",
      "           1       0.77      0.76      0.77        54\n",
      "\n",
      "    accuracy                           0.81       132\n",
      "   macro avg       0.80      0.80      0.80       132\n",
      "weighted avg       0.81      0.81      0.81       132\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.80      0.84        85\n",
      "           1       0.69      0.81      0.75        47\n",
      "\n",
      "    accuracy                           0.80       132\n",
      "   macro avg       0.79      0.80      0.79       132\n",
      "weighted avg       0.81      0.80      0.81       132\n",
      "\n",
      "average precision: 0.8171091605746751\n",
      "average recall: 0.8142383790571388\n",
      "average f1: 0.8138050506845449\n",
      "average accuracy: 0.8214513556618819\n"
     ]
    }
   ],
   "source": [
    "# specificity classification\n",
    "\n",
    "results = []\n",
    "accuracy = []\n",
    "\n",
    "n=5\n",
    "kf = KFold(n_splits=n, random_state=72, shuffle=True)\n",
    "# penalty='l1', C=1.0, solver='liblinear'\n",
    "# penalty='none'\n",
    "\n",
    "# penalty='l2', C=1.0, max_iter=100\n",
    "for train_index, test_index in kf.split(df_specificity_all):\n",
    "    train_documents = df_specificity_all.iloc[train_index]\n",
    "    test_documents = df_specificity_all.iloc[test_index]\n",
    "\n",
    "    # Unoptimized LR\n",
    "    # m_spec = LogisticRegression(penalty=None, max_iter=300)\n",
    "    # Optimized LR\n",
    "    m_spec = LogisticRegression(penalty='l1', C=5.0, solver='liblinear', max_iter=300)\n",
    "\n",
    "    m_spec.fit(train_documents['document_vector'].values.tolist(), train_documents.specificity)\n",
    "\n",
    "    # sns.histplot(m_spec.coef_[0], kde=True, binwidth=0.1)\n",
    "\n",
    "    pred_class = m_spec.predict(test_documents['document_vector'].values.tolist())\n",
    "\n",
    "    print(classification_report(test_documents.specificity, pred_class))\n",
    "    results.append(precision_recall_fscore_support(test_documents.specificity, pred_class, average='macro'))\n",
    "    accuracy.append(accuracy_score(test_documents.specificity, pred_class))\n",
    "\n",
    "avg_precision = np.mean([results[0][0], results[1][0], results[2][0], results[3][0], results[4][0]])\n",
    "avg_recall = np.mean([results[0][1], results[1][1], results[2][1], results[3][1], results[4][1]])\n",
    "avg_f = np.mean([results[0][2], results[1][2], results[2][2], results[3][2], results[4][2]])\n",
    "avg_acc = np.mean(accuracy)\n",
    "\n",
    "print(f\"average precision: {avg_precision}\")\n",
    "print(f\"average recall: {avg_recall}\")\n",
    "print(f\"average f1: {avg_f}\")\n",
    "print(f\"average accuracy: {avg_acc}\")\n",
    "\n",
    "result_specificity = {\"macro avg\":{\"average precision\" : avg_precision,\n",
    "                                  \"average recall\" : avg_recall,\n",
    "                                  \"average f1\" : avg_f,\n",
    "                                  \"average accuracy\" : avg_acc\n",
    "                                  }}\n",
    "result_specificity = pd.DataFrame(result_specificity).transpose()\n",
    "with pd.ExcelWriter(\"metrics_new.xlsx\", mode=\"a\", engine=\"openpyxl\", if_sheet_exists='replace') as writer:\n",
    "    result_specificity.to_excel(writer, sheet_name=\"glove_specificity_LR\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
