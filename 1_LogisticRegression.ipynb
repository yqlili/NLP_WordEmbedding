{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ThreadTheRipper\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ThreadTheRipper\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Tokenization\n",
      "Adding Alphabetic symbols and lowercase\n",
      "Adding Numerical symbols\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                  text  \\\n0    The accelerator programs have sub-portfolios o...   \n1    Also by means of BNDES Finem, we offer credit ...   \n2    Climate change Climate change exposes UPM to v...   \n3    Several tools and methodologies aimed at asses...   \n4    We worked with the UK government to accelerate...   \n..                                                 ...   \n395  At the beginning of 2019, VINCI Airports signe...   \n396  We have also signed up to the Partnership for ...   \n397  Suzano also is involved and spearheads externa...   \n398  Risks to the Group’s reputation Risks include ...   \n399  UBS is also involved in other activities to re...   \n\n                                                tokens  \n0    [the, accelerator, programs, have, sub, portfo...  \n1    [also, by, means, of, bndes, finem, we, offer,...  \n2    [climate, change, climate, change, exposes, up...  \n3    [several, tools, and, methodologies, aimed, at...  \n4    [we, worked, with, the, uk, government, to, ac...  \n..                                                 ...  \n395  [at, the, beginning, of, 2019, vinci, airports...  \n396  [we, have, also, signed, up, to, the, partners...  \n397  [suzano, also, is, involved, and, spearheads, ...  \n398  [risks, to, the, group, reputation, risks, inc...  \n399  [ubs, is, also, involved, in, other, activitie...  \n\n[400 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The accelerator programs have sub-portfolios o...</td>\n      <td>[the, accelerator, programs, have, sub, portfo...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Also by means of BNDES Finem, we offer credit ...</td>\n      <td>[also, by, means, of, bndes, finem, we, offer,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Climate change Climate change exposes UPM to v...</td>\n      <td>[climate, change, climate, change, exposes, up...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Several tools and methodologies aimed at asses...</td>\n      <td>[several, tools, and, methodologies, aimed, at...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>We worked with the UK government to accelerate...</td>\n      <td>[we, worked, with, the, uk, government, to, ac...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>395</th>\n      <td>At the beginning of 2019, VINCI Airports signe...</td>\n      <td>[at, the, beginning, of, 2019, vinci, airports...</td>\n    </tr>\n    <tr>\n      <th>396</th>\n      <td>We have also signed up to the Partnership for ...</td>\n      <td>[we, have, also, signed, up, to, the, partners...</td>\n    </tr>\n    <tr>\n      <th>397</th>\n      <td>Suzano also is involved and spearheads externa...</td>\n      <td>[suzano, also, is, involved, and, spearheads, ...</td>\n    </tr>\n    <tr>\n      <th>398</th>\n      <td>Risks to the Group’s reputation Risks include ...</td>\n      <td>[risks, to, the, group, reputation, risks, inc...</td>\n    </tr>\n    <tr>\n      <th>399</th>\n      <td>UBS is also involved in other activities to re...</td>\n      <td>[ubs, is, also, involved, in, other, activitie...</td>\n    </tr>\n  </tbody>\n</table>\n<p>400 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from pathlib import Path\n",
    "from transformers import TransfoXLTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sns\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 1: Transformer\n",
    "# 2: NLTK word tokenize\n",
    "# 3: WhiteSpace\n",
    "tokenization_method = 2\n",
    "\n",
    "# should links be removed or not\n",
    "preprocess_links = False\n",
    "\n",
    "# other preprocessing steps\n",
    "# baseline boolean gets set when all three are false, returns tokens directly\n",
    "# lowercasing and alphabetic symbols\n",
    "lower_alpha = True\n",
    "# numeric tokens\n",
    "numerical = True\n",
    "# currency & percentage symbols\n",
    "spec_char = False\n",
    "\n",
    "remove_stopwords = False\n",
    "lemmatize_tokens = False\n",
    "stemm_tokens = False\n",
    "\n",
    "if lemmatize_tokens and stemm_tokens:\n",
    "    raise Exception(\"Sorry, no lemmatization and stemming at the same time\")\n",
    "if not lower_alpha and not numerical and not spec_char:\n",
    "    # if all three steps are false it is more efficient to set baseline to True\n",
    "    baseline = True\n",
    "else:\n",
    "    baseline = False\n",
    "\n",
    "# Create a TransfoXLTokenizer object with the add_special_tokens parameter set to False\n",
    "tokenizer = TransfoXLTokenizer.from_pretrained(\"transfo-xl-wt103\", add_special_tokens=False)\n",
    "\n",
    "# Compile a regular expression pattern to match currency symbols and percentage signs\n",
    "currency_symbol_pattern = re.compile(r'[$€£₹]')\n",
    "percentage_pattern = re.compile(r'\\d*%|\\b%')\n",
    "# currency_pattern = re.compile(r'[$€£₹]?\\s?\\d+(?:[.,]\\d{3})*(?:[.,]\\d{2})?(?=[^\\d.,]|$)')\n",
    "\n",
    "# variables to indicate whether the logging was already printed once or not\n",
    "printed_mod_alpha = False\n",
    "printed_mod_num = False\n",
    "printed_mod_spec = False\n",
    "printed_mod_base = False\n",
    "\n",
    "# function to test different token modification techniques\n",
    "def lowercase_delete_special_characters(tokens):\n",
    "    modified_tokens = []\n",
    "    # variables to indicate whether the logging was already printed once or not\n",
    "    global printed_mod_alpha\n",
    "    global printed_mod_num\n",
    "    global printed_mod_spec\n",
    "    global printed_mod_base\n",
    "\n",
    "    # if no preprocessing was marked as true, return tokens directly to increase performance\n",
    "    if baseline:\n",
    "        if not printed_mod_base:\n",
    "            # print logging once\n",
    "            printed_mod_base = True\n",
    "            print(\"Baseline modifications\")\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    else:\n",
    "        # loop through tokens to modify them\n",
    "        for token in tokens:\n",
    "            # check whether preprocessing step is set to true and if token matches or not\n",
    "            if token.isalpha() and lower_alpha:\n",
    "                # if it matches, lowercase and append\n",
    "                modified_tokens.append(token.lower())\n",
    "                # print logging once\n",
    "                if not printed_mod_alpha:\n",
    "                    printed_mod_alpha = True\n",
    "                    print(\"Adding Alphabetic symbols and lowercase\")\n",
    "            # check whether preprocessing step is set to true and if token matches or not\n",
    "            elif token.isnumeric() and numerical:\n",
    "                modified_tokens.append(token)\n",
    "                if not printed_mod_num:\n",
    "                    printed_mod_num = True\n",
    "                    print(\"Adding Numerical symbols\")\n",
    "            # check whether preprocessing step is set to true and if token matches or not\n",
    "            # can be modified for other regular expression, modification to re.match also possible\n",
    "            elif spec_char and (re.search(currency_symbol_pattern, token) or re.search(percentage_pattern, token)):\n",
    "                # re.search(currency_pattern, token)\n",
    "                modified_tokens.append(token)\n",
    "                if not printed_mod_spec:\n",
    "                    printed_mod_spec = True\n",
    "                    print(\"Adding Currency and Percentage symbols\")\n",
    "\n",
    "        return modified_tokens\n",
    "\n",
    "def remove_links(string):\n",
    "    # Compile the regular expression pattern to match substrings that start with a slash or a string followed by a slash, and that may contain any number of additional slashes and text in between, followed by a string with a dot and some type of file type\n",
    "\n",
    "    # Pattern to match domain names\n",
    "    domain_pattern = re.compile(r'(?:(?:https?://)?(?:www\\.)?)?([\\w\\.]+\\.[a-z]+)')\n",
    "    # Pattern to match substrings like \"about-us/investors/pages/\"\n",
    "    path_pattern = re.compile(r'(\\S+\\/|\\/)(\\/\\S+\\/?)*\\S+\\.[a-z]+')\n",
    "\n",
    "    # Replace all links in the string with an empty string\n",
    "    string = re.sub(r'https?:\\/\\/\\S+', '', string)\n",
    "    # Replace all substrings that match the domain pattern or the path pattern with an empty string\n",
    "    clean_string = domain_pattern.sub('', path_pattern.sub('', string))\n",
    "    # Replace multiple spaces with a single space\n",
    "    clean_string = re.sub(' +', ' ', clean_string)\n",
    "\n",
    "    return clean_string\n",
    "\n",
    "# Filter out stop words\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def filter_stop_words(tokens):\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "# Create a lemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a function to convert POS tags to WordNet POS tags\n",
    "# needed to determine type of word in order to lemmatize it e.g. verb, noun\n",
    "def get_wordnet_pos(treebank_pos):\n",
    "    if treebank_pos.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_pos.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_pos.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_pos.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # default to noun if no match is found\n",
    "\n",
    "# Apply POS tagging and lemmatization to a list of words\n",
    "def lemmatize_words(words):\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    lemmatized_words = []\n",
    "    for word, tag in tagged_words:\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        lemmatized_words.append(lemmatizer.lemmatize(word, wn_tag))\n",
    "    return lemmatized_words\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "path = str(Path.cwd()) + '\\project_training.json'\n",
    "# print(path)\n",
    "# Read the JSON file\n",
    "with open(path, 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "# Load the JSON data into a dataframe\n",
    "df_train = pd.read_json(data)\n",
    "\n",
    "# Create a dataframes for the text and the climate label\n",
    "df_text = pd.DataFrame(df_train, columns=['text'])\n",
    "df_climate = pd.DataFrame(df_train, columns=['climate'])\n",
    "# Keep rows where df_climate is 'yes'\n",
    "df_filtered = df_train.loc[df_climate['climate'] == 'yes']\n",
    "\n",
    "# Split the filtered dataframe into separate dataframes\n",
    "df_text_climate_yes = pd.DataFrame(df_filtered, columns=['text'])\n",
    "df_sentiment = pd.DataFrame(df_filtered, columns=['sentiment'])\n",
    "df_commitment = pd.DataFrame(df_filtered, columns=['commitment'])\n",
    "df_specificity = pd.DataFrame(df_filtered, columns=['specificity'])\n",
    "# turn all labels into numerical labels\n",
    "\n",
    "df_climate = df_climate.replace({'yes': 1, 'no': 0})\n",
    "# opportunity/neutral/risk\n",
    "df_sentiment = df_sentiment.replace({'opportunity': 0, 'neutral': 1, 'risk': 2})\n",
    "# yes/no\n",
    "df_commitment = df_commitment.replace({'yes': 1, 'no': 0})\n",
    "# specific language/non-specific language\n",
    "df_specificity = df_specificity.replace({'spec': 1, 'non': 0})\n",
    "\n",
    "path = str(Path.cwd()) + '\\project_validation.json'\n",
    "with open(path, 'r') as f_test:\n",
    "    data_test = f_test.read()\n",
    "\n",
    "# Load the JSON data into a dataframe\n",
    "df_test = pd.read_json(data_test)\n",
    "df_text_test = pd.DataFrame(df_test, columns=['text'])\n",
    "df_climate_test = pd.DataFrame(df_test, columns=['climate'])\n",
    "# Keep rows where df_climate is 'yes'\n",
    "df_filtered_test = df_test.loc[df_climate_test['climate'] == 'yes']\n",
    "df_text_test_climate_yes = pd.DataFrame(df_filtered_test, columns=['text'])\n",
    "df_sentiment_test = pd.DataFrame(df_filtered_test, columns=['sentiment'])\n",
    "df_commitment_test = pd.DataFrame(df_filtered_test, columns=['commitment'])\n",
    "df_specificity_test = pd.DataFrame(df_filtered_test, columns=['specificity'])\n",
    "# same for climate classification text data\n",
    "df_climate_test = df_climate_test.replace({'yes': 1, 'no': 0})\n",
    "# opportunity/neutral/risk\n",
    "df_sentiment_test = df_sentiment_test.replace({'opportunity': 0, 'neutral': 1, 'risk': 2})\n",
    "# yes/no\n",
    "df_commitment_test = df_commitment_test.replace({'yes': 1, 'no': 0})\n",
    "# specific language/non-specific language\n",
    "df_specificity_test = df_specificity_test.replace({'spec': 1, 'non': 0})\n",
    "\n",
    "# remove links\n",
    "if preprocess_links:\n",
    "    print(\"Removing Links...\")\n",
    "    df_text['text'] = df_text['text'].apply(remove_links)\n",
    "    df_text_climate_yes['text'] = df_text_climate_yes['text'].apply(remove_links)\n",
    "    df_text_test['text'] = df_text_test['text'].apply(remove_links)\n",
    "    df_text_test_climate_yes['text'] = df_text_test_climate_yes['text'].apply(remove_links)\n",
    "\n",
    "# tokenization methods\n",
    "if tokenization_method == 1:\n",
    "    print(\"Transformer Tokenization\")\n",
    "    df_text['tokens'] = df_text['text'].apply(tokenizer.tokenize)\n",
    "    df_text_climate_yes['tokens'] = df_text_climate_yes['text'].apply(tokenizer.tokenize)\n",
    "    df_text_test['tokens'] = df_text_test['text'].apply(tokenizer.tokenize)\n",
    "    df_text_test_climate_yes['tokens'] = df_text_test_climate_yes['text'].apply(tokenizer.tokenize)\n",
    "elif tokenization_method == 2:\n",
    "    print(\"NLTK Tokenization\")\n",
    "    df_text['tokens'] = df_text['text'].apply(nltk.word_tokenize)\n",
    "    df_text_climate_yes['tokens'] = df_text_climate_yes['text'].apply(nltk.word_tokenize)\n",
    "    df_text_test['tokens'] = df_text_test['text'].apply(nltk.word_tokenize)\n",
    "    df_text_test_climate_yes['tokens'] = df_text_test_climate_yes['text'].apply(nltk.word_tokenize)\n",
    "elif tokenization_method == 3:\n",
    "    print(\"WhiteSpace Tokenization\")\n",
    "    df_text['tokens'] = df_text['text'].str.split()\n",
    "    df_text_climate_yes['tokens'] = df_text_climate_yes['text'].str.split()\n",
    "    df_text_test['tokens'] = df_text_test['text'].str.split()\n",
    "    df_text_test_climate_yes['tokens'] = df_text_test_climate_yes['text'].str.split()\n",
    "\n",
    "# preprocessing\n",
    "df_text['tokens'] = df_text['tokens'].apply(lowercase_delete_special_characters)\n",
    "df_text_climate_yes['tokens'] = df_text_climate_yes['tokens'].apply(lowercase_delete_special_characters)\n",
    "df_text_test['tokens'] = df_text_test['tokens'].apply(lowercase_delete_special_characters)\n",
    "df_text_test_climate_yes['tokens'] = df_text_test_climate_yes['tokens'].apply(lowercase_delete_special_characters)\n",
    "\n",
    "\n",
    "# stopwords\n",
    "if remove_stopwords:\n",
    "    print(\"Removing Stopwords...\")\n",
    "    df_text['tokens'] = df_text['tokens'].apply(filter_stop_words)\n",
    "    df_text_climate_yes['tokens'] = df_text_climate_yes['tokens'].apply(filter_stop_words)\n",
    "    df_text_test['tokens'] = df_text_test['tokens'].apply(filter_stop_words)\n",
    "    df_text_test_climate_yes['tokens'] = df_text_test_climate_yes['tokens'].apply(filter_stop_words)\n",
    "\n",
    "# lemmatization\n",
    "if lemmatize_tokens:\n",
    "    print(\"Lemmatizing...\")\n",
    "    # Apply lemmatization to the 'tokens' column in your DataFrame\n",
    "    df_text['tokens'] = df_text['tokens'].apply(lambda x: lemmatize_words(x))\n",
    "    df_text_climate_yes['tokens'] = df_text_climate_yes['tokens'].apply(lambda x: lemmatize_words(x))\n",
    "    df_text_test['tokens'] = df_text_test['tokens'].apply(lambda x: lemmatize_words(x))\n",
    "    df_text_test_climate_yes['tokens'] = df_text_test_climate_yes['tokens'].apply(lambda x: lemmatize_words(x))\n",
    "\n",
    "# stemming\n",
    "if stemm_tokens:\n",
    "    print(\"Stemming...\")\n",
    "    df_text['tokens'] = df_text['tokens'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "    df_text_climate_yes['tokens'] = df_text_climate_yes['tokens'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "    df_text_test['tokens'] = df_text_test['tokens'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "    df_text_test_climate_yes['tokens'] = df_text_test_climate_yes['tokens'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "df_text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [],
   "source": [
    "# Self-trained word embedding, only run if self-trained word embedding needed\n",
    "\n",
    "import gensim\n",
    "# read the unlabelled data\n",
    "path = Path.cwd() / 'NLP_project_unlabelled_slim.txt'\n",
    "with open(path, 'r', encoding='utf-8') as file:\n",
    "    text_unlabelled = file.read()\n",
    "\n",
    "# split each paragraph into one position of a list\n",
    "text_unlabelled_list = text_unlabelled.split('\\n')\n",
    "# save unlabelled paragraphs into dataframe\n",
    "df_text_unlabelled = pd.DataFrame(text_unlabelled_list, columns=['text'])\n",
    "# concat with training data\n",
    "df_text_unlabelled = pd.DataFrame(pd.concat([df_text.text, df_text_unlabelled.text], ignore_index=True))\n",
    "# drop duplicates\n",
    "df_text_unlabelled = df_text_unlabelled.drop_duplicates()\n",
    "# drop last row as it is empty\n",
    "df_text_unlabelled.drop([100400], inplace=True)\n",
    "\n",
    "# tokenize and preprocess using NLTK, preprocessing is still dependant on set variables at beginning of Notebook\n",
    "df_text_unlabelled['tokens'] = df_text_unlabelled['text'].apply(nltk.word_tokenize)\n",
    "df_text_unlabelled['tokens'] = df_text_unlabelled['tokens'].apply(lowercase_delete_special_characters)\n",
    "\n",
    "# Convert the data to a list of lists of tokens\n",
    "data_embedding = list(df_text_unlabelled['tokens'])\n",
    "\n",
    "# Initialize a Word2Vec model with an embedding size of 300, and 5 epochs\n",
    "own_embedding = gensim.models.Word2Vec(sentences=data_embedding, vector_size=300, min_count=1, window=5, epochs=5)\n",
    "# self-trained word embedding\n",
    "model_word2vec = own_embedding.wv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# load GloVe word embedding\n",
    "model_word2vec = KeyedVectors.load_word2vec_format('glove.6B.300d.txt', binary=False, no_header=True)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [],
   "source": [
    "def document_vector_func(doc):\n",
    "    # Create document vectors by averaging word vectors, Remove out-of-vocabulary words\n",
    "    doc = [model_word2vec.get_vector(word) for word in doc if word in model_word2vec.index_to_key]\n",
    "    doc = np.vstack(doc)\n",
    "    return np.mean(doc, axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "outputs": [
    {
     "data": {
      "text/plain": "                                       document_vector  climate\n0    [-0.091688745, 0.07141805, -0.12825042, -0.115...        0\n1    [-0.10824829, 0.11942579, -0.011191578, -0.206...        1\n2    [-0.078438394, 0.058795307, 0.0060408427, -0.1...        1\n3    [-0.09102161, 0.16441008, 0.043849185, -0.1823...        1\n4    [-0.030263793, 0.057995267, -0.07471928, -0.19...        1\n..                                                 ...      ...\n395  [-0.11305666, 0.10444998, 0.05659384, -0.19879...        0\n396  [-0.092985146, 0.12477428, 0.0066316435, -0.20...        1\n397  [-0.01266073, 0.009422414, -0.082624316, -0.18...        1\n398  [-0.13721633, 0.1194861, 0.040766872, -0.19293...        1\n399  [-0.06849083, 0.07159914, -0.084468, -0.195207...        1\n\n[400 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>document_vector</th>\n      <th>climate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[-0.091688745, 0.07141805, -0.12825042, -0.115...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[-0.10824829, 0.11942579, -0.011191578, -0.206...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[-0.078438394, 0.058795307, 0.0060408427, -0.1...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[-0.09102161, 0.16441008, 0.043849185, -0.1823...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[-0.030263793, 0.057995267, -0.07471928, -0.19...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>395</th>\n      <td>[-0.11305666, 0.10444998, 0.05659384, -0.19879...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>396</th>\n      <td>[-0.092985146, 0.12477428, 0.0066316435, -0.20...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>397</th>\n      <td>[-0.01266073, 0.009422414, -0.082624316, -0.18...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>398</th>\n      <td>[-0.13721633, 0.1194861, 0.040766872, -0.19293...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>399</th>\n      <td>[-0.06849083, 0.07159914, -0.084468, -0.195207...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>400 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate document vectors using defined function above\n",
    "df_text['document_vectors'] = df_text['tokens'].apply(document_vector_func)\n",
    "df_text_test['document_vectors'] = df_text_test['tokens'].apply(document_vector_func)\n",
    "df_text_climate_yes['document_vectors'] = df_text_climate_yes['tokens'].apply(document_vector_func)\n",
    "df_text_test_climate_yes['document_vectors'] = df_text_test_climate_yes['tokens'].apply(document_vector_func)\n",
    "\n",
    "# convert to list\n",
    "document_vectors = df_text.document_vectors.tolist()\n",
    "document_vectors_test = df_text_test.document_vectors.tolist()\n",
    "document_vectors_climate_yes = df_text_climate_yes.document_vectors.tolist()\n",
    "document_vectors_test_climate_yes = df_text_test_climate_yes.document_vectors.tolist()\n",
    "\n",
    "# save document vectors in appropriate dataframe, so that it can be used when classifying\n",
    "df_climate['document_vector'] = document_vectors\n",
    "df_climate_test['document_vector'] = document_vectors_test\n",
    "df_sentiment['document_vector'] = document_vectors_climate_yes\n",
    "df_sentiment_test['document_vector'] = document_vectors_test_climate_yes\n",
    "df_commitment['document_vector'] = document_vectors_climate_yes\n",
    "df_commitment_test['document_vector'] = document_vectors_test_climate_yes\n",
    "df_specificity['document_vector'] = document_vectors_climate_yes\n",
    "df_specificity_test['document_vector'] = document_vectors_test_climate_yes\n",
    "\n",
    "# update the dataframe column order (document_vector, label)\n",
    "df_climate = df_climate.iloc[:, [1, 0]]\n",
    "df_climate_test = df_climate_test.iloc[:, [1, 0]]\n",
    "df_sentiment = df_sentiment.iloc[:, [1, 0]]\n",
    "df_sentiment_test = df_sentiment_test.iloc[:, [1, 0]]\n",
    "df_commitment = df_commitment.iloc[:, [1, 0]]\n",
    "df_commitment_test = df_commitment_test.iloc[:, [1, 0]]\n",
    "df_specificity = df_specificity.iloc[:, [1, 0]]\n",
    "df_specificity_test = df_specificity_test.iloc[:, [1, 0]]\n",
    "\n",
    "\n",
    "df_climate_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.59      0.66        71\n",
      "           1       0.92      0.95      0.93       329\n",
      "\n",
      "    accuracy                           0.89       400\n",
      "   macro avg       0.83      0.77      0.80       400\n",
      "weighted avg       0.88      0.89      0.89       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# climate classification\n",
    "\n",
    "m = LogisticRegression(penalty=None, max_iter=300)\n",
    "# fit training data\n",
    "m.fit(df_climate.document_vector.values.tolist(), df_climate.climate)\n",
    "\n",
    "# sns.histplot(m.coef_[0], kde=True, binwidth=0.1)\n",
    "\n",
    "# predict test data\n",
    "pred_class = m.predict(df_climate_test.document_vector.values.tolist())\n",
    "\n",
    "# compare result of prediction for test data with actual labels of test data\n",
    "print(classification_report(df_climate_test.climate, pred_class))\n",
    "# write the result into an Excel file, Hint: Excel File has to exist before data can be written\n",
    "result_climate = pd.DataFrame(classification_report(df_climate_test.climate, pred_class, output_dict=True)).transpose()\n",
    "with pd.ExcelWriter(\"metrics_new.xlsx\", mode=\"a\", engine=\"openpyxl\", if_sheet_exists='replace') as writer:\n",
    "    result_climate.to_excel(writer, sheet_name=\"glove_climate_LR\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.78      0.71        81\n",
      "           1       0.73      0.62      0.67       136\n",
      "           2       0.81      0.84      0.82       112\n",
      "\n",
      "    accuracy                           0.74       329\n",
      "   macro avg       0.73      0.75      0.74       329\n",
      "weighted avg       0.74      0.74      0.73       329\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sentiment classification\n",
    "\n",
    "m = LogisticRegression(penalty=None, max_iter=300)\n",
    "# fit training data\n",
    "m.fit(df_sentiment.document_vector.values.tolist(), df_sentiment.sentiment)\n",
    "\n",
    "# sns.histplot(m.coef_[0], kde=True, binwidth=0.1)\n",
    "\n",
    "# predict test data\n",
    "pred_class = m.predict(df_sentiment_test.document_vector.values.tolist())\n",
    "\n",
    "# compare result of prediction for test data with actual labels of test data\n",
    "print(classification_report(df_sentiment_test.sentiment, pred_class))\n",
    "# write the result into an Excel file, Hint: Excel File has to exist before data can be written\n",
    "result_climate = pd.DataFrame(classification_report(df_sentiment_test.sentiment, pred_class, output_dict=True)).transpose()\n",
    "with pd.ExcelWriter(\"metrics_new.xlsx\", mode=\"a\", engine=\"openpyxl\", if_sheet_exists='replace') as writer:\n",
    "    result_climate.to_excel(writer, sheet_name=\"glove_sentiment_LR\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.74      0.79       190\n",
      "           1       0.69      0.81      0.75       139\n",
      "\n",
      "    accuracy                           0.77       329\n",
      "   macro avg       0.77      0.77      0.77       329\n",
      "weighted avg       0.78      0.77      0.77       329\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# commitment classification\n",
    "\n",
    "m = LogisticRegression(penalty=None, max_iter=300)\n",
    "# fit training data\n",
    "m.fit(df_commitment.document_vector.values.tolist(), df_commitment.commitment)\n",
    "\n",
    "# sns.histplot(m.coef_[0], kde=True, binwidth=0.1)\n",
    "\n",
    "# predict test data\n",
    "pred_class = m.predict(df_commitment_test.document_vector.values.tolist())\n",
    "\n",
    "# compare result of prediction for test data with actual labels of test data\n",
    "print(classification_report(df_commitment_test.commitment, pred_class))\n",
    "# write the result into an Excel file, Hint: Excel File has to exist before data can be written\n",
    "result_climate = pd.DataFrame(classification_report(df_commitment_test.commitment, pred_class, output_dict=True)).transpose()\n",
    "with pd.ExcelWriter(\"metrics_new.xlsx\", mode=\"a\", engine=\"openpyxl\", if_sheet_exists='replace') as writer:\n",
    "    result_climate.to_excel(writer, sheet_name=\"glove_commitment_LR\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.78      0.82       197\n",
      "           1       0.72      0.82      0.76       132\n",
      "\n",
      "    accuracy                           0.80       329\n",
      "   macro avg       0.79      0.80      0.79       329\n",
      "weighted avg       0.81      0.80      0.80       329\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# specificity classification\n",
    "\n",
    "m = LogisticRegression(penalty=None, max_iter=300)\n",
    "# fit training data\n",
    "m.fit(df_specificity.document_vector.values.tolist(), df_specificity.specificity)\n",
    "\n",
    "# sns.histplot(m.coef_[0], kde=True, binwidth=0.1)\n",
    "\n",
    "# predict test data\n",
    "pred_class = m.predict(df_specificity_test.document_vector.values.tolist())\n",
    "\n",
    "# compare result of prediction for test data with actual labels of test data\n",
    "print(classification_report(df_specificity_test.specificity, pred_class))\n",
    "# write the result into an Excel file, Hint: Excel File has to exist before data can be written\n",
    "result_climate = pd.DataFrame(classification_report(df_specificity_test.specificity, pred_class, output_dict=True)).transpose()\n",
    "with pd.ExcelWriter(\"metrics_new.xlsx\", mode=\"a\", engine=\"openpyxl\", if_sheet_exists='replace') as writer:\n",
    "    result_climate.to_excel(writer, sheet_name=\"glove_specificity_LR\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
